\chapter{Statistical Inference}
\label{chp:statisticalinference}

\begin{multicols}{2}[\subsubsection*{Contents of this chapter}]
   \printcontents{}{1}{\setcounter{tocdepth}{2}}
\end{multicols}

\input{./chapters/sections/stats_modelsinference.tex}

\input{./chapters/sections/stats_functionals.tex}

\input{./chapters/sections/stats_bootstrap.tex}

\input{./chapters/sections/stats_parametricinference.tex}


\input{./chapters/sections/stats_directional.tex}

\input{./chapters/sections/stats_features.tex}

\input{./chapters/sections/stats_fattails.tex}



\section{$R^2$ Value}

\section{Regression Diagnostics}

\section{t-Statistics}

\section{AIC and BIC}

\section{Factor Regression}

\section{Factor Models}


\section{How to Combine Estimates with Different Uncertainties}
A series of classic papers exist on this subject.  


\section{How to Tackle Very High Dimensional Feature Spaces}
In practice, high-dimensional feature spaces amplify the risk of overfitting. The answer, framed in the most general way, is to introduce limitations on the family of functions that are fit by the estimator of choice. The generic methods are regularization (i.e. penalizing variance), variable selection (i.e. filtering out irrelevant features) and dimensionality reduction through feature transformation (i.e. finding a lower-rank representation of the data). It's not possible to draw neat distinctions between this methods in terms of their effects. For example, regularization often amounts to variable selection through soft (or hard) thresholding. Drop out regularization in neural networks biases the model towards particular types of feature transformations.



