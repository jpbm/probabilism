\section{Logistic Regression}

Logistic Regression is a generalized linear model suitable for fitting probabilities, i.e. dependent variables that vary $y\in (0,1)$. 

\subsection{Binary Response Case}
Assume that the dependent variable is a scalar and not a vector (i.e. only a single probability is being predicted). Then, assume that the dependence of that probability on the exogenous variable $\mathbf{x}$ and the parameters of the model $\mathbf{b}$ is given by:

\begin{equation}
p(\mathbf{x}) = \frac{1}{1+\exp(\mathbf{b\cdot x})}
\end{equation}

This is, of course, the sigmoid function, which also shows up as the Fermi-Dirac distribution. It creates a symmetric mapping from the interval $(-\infty,\infty)$ to $(0,1)$. It amounts to treating the log-odds as a linear function of the coefficients: 

\begin{equation}
\ln\left(\frac{p(\mathbf{x})}{1-p(\mathbf{x})}\right) = \mathbf{b \cdot x}
\end{equation}

Equivalently:

\begin{equation}
\frac{p(\mathbf{x})}{1-p(\mathbf{x})} = \prod_j e^{b_j x_j}
\end{equation}

So the model assumes that different values of the independent variables will have a multiplicative effect on the likelihood ratio of the two binary outcomes. There is a deep reason for this choice of mapping, which is why it shows up as the Fermi-Dirac distribution in physics. In physics, it emerges from looking at the probability of energy levels being occupied by fermions. This is done by considering the number of equally possible configurations that correspond to a set of probabilities of a certain energy level being occupied. The Fermi-Dirac distribution is the distribution that maximizes the number of equally possible configurations, and so I suppose the sigmoid function has some maximum entropy property, which makes it the "maximally naive guess". 

Assuming that the data is generated by independent trials, the likelihood of observing the data is given by multiplying the probbility of all the datapoints together:

\begin{equation}
\mathscr{L}(\mathrm{data}|p(\mathbf{x})) = \prod_{y_i=1} p(\mathbf{x_i}) \prod_{y_i=0} (1-p(\mathbf{x_i}))
\end{equation}

Strictly speaking, I think there should be a factor of $1/N!$ here to account for the probability of seeing a specific permutation of the individual data points (which are distinguishable), but that factor will be independent of $p(\mathbf{x})$ and so it will not matter to estimation. In the literature I haven't seen a factor show up.

The expression $-2 \ln(\mathscr{L})$ is called the \textit{deviance}, which is the analog to residual sum of squares in ordinary linear regression. In OLS, the log likelihood is: $\ln\mathscr{L} = -\sum|\hat{y}-y|^2 + ...$, while the deviance is $-2\ln\mathscr{L} = -\sum_i \ln\left[\left(p(\mathbf{x_i})^{y_i}(1-p(\mathbf{x_i}))^{y_i-1}\right)^2\right]$.

Logistic regression is then fit by finding the coefficients that minimize the deviance / maximize the likelihood.

Question for another day: in OLS, there is an assumption that the data will deviate from the mean, where the probability of observing a certain deviation is given by a normal distribution. This has the consequence, for example, that outliers have an outsize impact on the fitted mean, because the normal distribution otherwise models them as exceedingly unlikely. If the normal distribution is the error distribution for OLS, what is the error distribution for logistic regression? I believe it looks like the Fermi-Dirac Distribution, where the Fermi-Energy is the 50\% decision boundary. 

\subsection{Pseudo-$R^2$}
The deviance can be used to create an error metric analogous to $R^2$ for OLS, named Pseudo-$R^2$. It compares the deviance to the deviance of simply assuming the average probability based on the class balance.

\begin{equation}
\mathrm{pseudo}-R^2 = 1 - \frac{\mathrm{deviance}}{\mathrm{deviance\ when\ }p=\mathrm{\ average\ class}}
\end{equation}
