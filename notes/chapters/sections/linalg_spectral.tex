\section{$\mathbf{A} = \mathbf{V\Lambda V^{-1}}$ Spectral Theorems, Diagonalization}
\label{sec:diagonalization}

Spectral theorems deal with diagonalizable linear operators. 
\\

A diagonalization of a matrix $\mathbf{A}$ is always possible when a matrix is square, and refers to a decomposition of the matrix into the matrix of eigenvectors $\mathbf{V}$ and eigenvalues $\mathbf{\Lambda}$ as 

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda V^{-1}} 
\end{equation}

\subsection{$\mathbf{A} = \mathbf{V\Lambda V}^T$ Eigendecomposition of Symmetric Matrices}
A hermitian matrix $\mathbf{A}$ has orthogonal eigenvectors, which means that $\mathbf{V}$ is unitary, meaning that $\mathbf{V}^{-1} = \mathbf{V}^{\dagger}$. In that case, the diagonalization is:

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda V}^{\dagger} = \left[\begin{array}{cccc}
\vrule&\vrule&\hdots&\vrule\\
v_1&v_2&\ddots&v_n\\
\vrule&\vrule&\hdots&\vrule
\end{array}\right]\left[\begin{array}{cccc}
\lambda_1&0&\hdots&0\\ 
0&\lambda_2&\hdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&\hdots&\hdots&\lambda_n
\end{array}\right]
\left[
\begin{array}{ccc}
\rule[.5ex]{3.5em}{0.4pt}&v_1^{\dagger}&\rule[.5ex]{3.5em}{0.4pt}\\
\rule[.5ex]{3.5em}{0.4pt}&v_2^{\dagger}&\rule[.5ex]{3.5em}{0.4pt}\\
\vdots&\ddots&\vdots\\
\rule[.5ex]{3.5em}{0.4pt}&v_n^{\dagger}&\rule[.5ex]{3.5em}{0.4pt}\\
\end{array}
\right]
\end{equation}

Which is the same as saying that all hermitian matrices are \textit{similar} to a diagonal matrix (cf. section \ref{sec:similiarity}, two matrices $\mathbf{A}$ and $\mathbf{B}$ are similar if they are transmutable using unitary transformations as $\mathbf{A} = \mathbf{UBU^{\dagger}}$). The diagonal representation also shows that in order for $\mathbf{A}$ to satisfy the hermitian property $\mathbf{A}^{\dagger}=\mathbf{A}$ its eigenvalues $\lambda_i$ must be real. Further, it means that if $\mathbf{A}$ is hermitian, $\mathbf{A}$ can be written in terms of projections on the eigenvectors:

\begin{equation}
\mathbf{A} = \sum_i \lambda_i (v_i \otimes v_i)
\end{equation}

Where $\otimes$ is the (complex) outer product $v_i\otimes v_i = v_i v_i^{\dagger}$. I have seen the existence of this representation of a hermitian matrix be described as synonmous with \textit{spectral theorem}. Since the eigenvectors $\mathbf{V}$ are an orthonormal basis, $\sum_i v_i \otimes v_i = \mathbb{I}$.


\subsection{$\mathbf{H} = \mathbf{U\Lambda U}^T$ Eigendecomposition of Hermitian Matrices}

Similarly, a hermitian matrix $\mathbf{H} \in \mathbb{C}^{n\times n}$ (the complex equivalent to a symmetric matrix) has real eigenvalues and the matrix of eigenvectors is unitary, so that $\mathbf{H} = \mathbf{U\Lambda U}^T$.


\subsection{Eigenvalue Sensitivity and Accuracy}

\subsubsection{General Case}

In general, the values of the eigenvalues of a square matrix $\mathbf{A}$ may vary wildly under a slight cange of $\mathbf{A} \rightarrow \mathbf{A}+\delta\mathbf{A}$. The sensitivity of the eigenvalues to a change in $\mathbf{A}$ can be investigated using matrix norms \cite{mathworkseig}. Let $||\cdot||$ denote a submultiplicative matrix norm, then:

\begin{equation}
\begin{array}{rl}
\Lambda + \delta\Lambda &= \mathbf{X^{-1}}\left( \mathbf{A} + \delta\mathbf{A} \right)\mathbf{X} \\
\delta\Lambda &= \mathbf{X^{-1}} \delta \mathbf{A} \mathbf{X}\\
||\delta\Lambda || &= ||\mathbf{X^{-1}} \delta \mathbf{A} \mathbf{X} || \leq  ||\mathbf{X^{-1}}|| ||\mathbf{X}|| ||\delta\mathbf{A}||
\end{array}
\end{equation}

When $||\cdot||$ is chosen to be the operator norm with respect to $L^2$, $||\cdot||_{(2)}||$, then $||\mathbf{X^{-1}}|| = \sigma_1$ and $||\mathbf{X}|| = \frac{1}{\sigma_n}$ where $\sigma_1$ and $\sigma_2$ are the square roots of the largest and the smallest eigenvalue of $\mathbf{X^{\dagger}}\mathbf{X}$ respectively (cf. section \ref{sec:norms} on matrix norms). In that case, the sensitivity of the eigenvalues to a change in $\mathbf{A}$ is:

\begin{equation}
||\delta\mathbf{\Lambda}||_{(2)} \leq \frac{\sigma_1}{\sigma_n} ||\delta\mathbf{A}||_{(2)} = \kappa(\mathbf{X})||\delta\mathbf{A}||_{(2)}
\end{equation}

Where $\kappa(\mathbf{X})$ is the conditioning number of the matrix $\mathbf{X}$. Upper bounds on the error on individual eigenvalues can also be derived quite easily, which is shown in \ref{mathworkseig} pp 10-12.

\subsubsection{Hermitian Matrices}
For hermitian (or orthogonal) matrices, the conditioning number for the individual eigenvalues  $\kappa(\lambda_i,\mathbf{H}) = 1$, so that the error on an individual eigenvalue $||\lambda_i||_{(2)} \leq \kappa(\lambda_i,\mathbf{H}) ||\mathbf{H}||_{(2)} = 1\times ||\mathbf{H}||_{(2)}$. 