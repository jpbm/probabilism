\section{$\mathbf{A} = \mathbf{V\Lambda V^{-1}}$ Spectral Theorems, Diagonalization}
\label{sec:diagonalization}

Spectral theorems deal with diagonalizable linear operators. 
\\

A diagonalization of a matrix matrix $\mathbf{A}$ is always possible when a matrix is square, and refers to a decomposition of the matrix into the matrix of eigenvectors $\mathbf{V}$ and eigenvalues $\mathbf{\Lambda}$ as 

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda V^{-1}} 
\end{equation}

\subsection{$\mathbf{A} = \mathbf{V\Lambda V}^T$ Eigendecomposition of Symmetric Matrices}
A symmetric matrix $\mathbf{A}$ has orthogonal eigenvectors so that $\mathbf{V}^{-1} = \mathbf{V}^T$ and real eigenvalues. In that case, the diagonalization is:

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda V}^T = \left[\begin{array}{cccc}
\vrule&\vrule&\hdots&\vrule\\
v_1&v_2&\ddots&v_n\\
\vrule&\vrule&\hdots&\vrule
\end{array}\right]\left[\begin{array}{cccc}
\lambda_1&0&\hdots&0\\ 
0&\lambda_2&\hdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&\hdots&\hdots&\lambda_n
\end{array}\right]
\left[
\begin{array}{ccc}
\rule[.5ex]{3.5em}{0.4pt}&v_1&\rule[.5ex]{3.5em}{0.4pt}\\
\rule[.5ex]{3.5em}{0.4pt}&v_2&\rule[.5ex]{3.5em}{0.4pt}\\
\vdots&\ddots&\vdots\\
\rule[.5ex]{3.5em}{0.4pt}&v_n&\rule[.5ex]{3.5em}{0.4pt}\\
\end{array}
\right]
\end{equation}

This means that  enables the expression of $\mathbf{A}$ in terms of projections on the eigenvectors:

\begin{equation}
\mathbf{A} = \sum_i \lambda_i (v_i \otimes v_i)
\end{equation}

Where $\otimes$ is the outer product. Since the eigenvectors are an orthonormal basis, $\sum_i v_i \otimes v_i = \mathbb{I}$.


\subsection{$\mathbf{H} = \mathbf{U\Lambda U}^T$ Eigendecomposition of Hermitian Matrices}

Similarly, a hermitian matrix $\mathbf{H} \in \mathbb{C}^{n\times n}$ (the complex equivalent to a symmetric matrix) has real eigenvalues and the matrix of eigenvectors is unitary, so that $\mathbf{H} = \mathbf{U\Lambda U}^T$.


\subsection{Eigenvalue Sensitivity and Accuracy}

In general, the values of the eigenvalues of a square matrix $\mathbf{A}$ may vary wildly under a slight cange of $\mathbf{A} \rightarrow \mathbf{A}+\delta\mathbf{A}$. The sensitivity of the eigenvalues to a change in $\mathbf{A}$ can be investigated using matrix norms \cite{mathworkseig}. Let $||\cdot||$ denote a submultiplicative matrix norm, then:

\begin{equation}
\begin{array}{rl}
\Lambda + \delta\Lambda &= \mathbf{X^{-1}}\left( \mathbf{A} + \delta\mathbf{A} \right)\mathbf{X} \\
\delta\Lambda &= \mathbf{X^{-1}} \delta \mathbf{A} \mathbf{X}\\
||\delta\Lambda || &= ||\mathbf{X^{-1}} \delta \mathbf{A} \mathbf{X} || \leq  ||\mathbf{X^{-1}}|| ||\mathbf{X}|| ||\delta\mathbf{A}||
\end{array}
\end{equation}

When $||\cdot||$ is chosen to be the operator norm with respect to $L^2$, $||\cdot||_{(2)}||$, then $||\mathbf{X^{-1}}|| = \sigma_1$ and $||\mathbf{X}|| = \frac{1}{\sigma_n}$ where $\sigma_1$ and $\sigma_2$ are the square roots of the largest and the smallest eigenvalue of $\mathbf{X^{\dagger}}\mathbf{X}$ respectively (cf. section \ref{sec:norms} on matrix norms). In that case, the sensitivity of the eigenvalues to a change in $\mathbf{A}$ is:

\begin{equation}
||\delta\mathbf{\Lambda}||_{(2)} \leq \frac{\sigma_1}{\sigma_n} ||\delta\mathbf{A}||_{(2)} = \kappa(\mathbf{X})||\delta\mathbf{A}||_{(2)}
\end{equation}

Where $\kappa(\mathbf{X})$ is the conditioning number of the matrix $\mathbf{X}$. 

