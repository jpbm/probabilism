\section{Vector and Matrix Derivatives}

Derivatives involving matrices and vectors can look nonintuitive when the usual symbolic matrix notation is used, but can be derived handily when index notation is used. A very concise and helpful resource for this is \citeasnoun{barnesmatrixdiff}. 


\subsection{Jacobian}
It is particularly helpful to remember the Jacobian, which is the derivative of a function with respect of a vector. The Jacobian of some function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is:

\begin{equation}
\frac{\mathrm{d}  \mathbf{f}(\mathbf{x})}{\mathrm{d} \mathbf{x}}=\left[\frac{\partial \mathbf{f}}{\partial x_1}, \hdots, \frac{\partial \mathbf{f}}{\partial x_n} \right]=\left[\begin{array}{ccc}
\frac{\partial  f_1}{\partial  x_1} & \hdots & \frac{\partial  f_1}{\partial  x_n} \\
\vdots & \vdots & \vdots \\
\frac{\partial  f_m}{\partial  x_1} & \hdots & \frac{\partial  f_m}{\partial  x_n} \\
\end{array}\right]
\end{equation}

I enjoy writing the gradient $\frac{\mathrm{d}}{\mathrm{d}\mathbf{x}}$ as $\nabla_\mathbf{x}$. The relationships below can all be derived as applications of the Jacobian.

\begin{equation}
\begin{array}{l}
\nabla_\mathbf{x} \left(\mathbf{u}^T\mathbf{x}\right) = \left[\frac{\partial }{\partial x_1}\left(\sum_i u_i x_i\right),...,\frac{\partial }{\partial x_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\nabla_\mathbf{x} \left(\mathbf{x}^T\mathbf{u}\right) = \left[\frac{\partial }{\partial x_1}\left(\sum_i u_i x_i\right),...,\frac{\partial }{\partial x_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\nabla_\mathbf{x} \left(\mathbf{x}^T\mathbf{x}\right) = \left[\frac{\partial }{\partial x_1}\left(\sum_i x_i^2\right),...,\frac{\partial }{\partial x_n}\left(\sum_i x_i^2\right)\right] = 2\mathbf{x}^T\\
\\
\nabla_\mathbf{x} \left(\mathbf{Ax}\right) = \left[
\begin{array}{ccc} 
\underbrace{\frac{\partial }{\partial x_1}\left(\sum_i A_{1i} x_i\right)}_{A_{11}} &...& \underbrace{\frac{\partial }{\partial x_n}\left(\sum_i A_{1i} x_i\right)}_{A_1n}\\
\vdots&\vdots&\vdots\\
\underbrace{\frac{\partial }{\partial x_1}\left(\sum_i A_{ni} x_i\right)}_{A_{n1}} &...& \underbrace{\frac{\partial }{\partial x_n}\left(\sum_i A_{ni} x_i\right)}_{A_{nn}}\\
\end{array}\right] = \mathbf{A}
\end{array}
\end{equation}


\subsection{Inverse Function Theorem}
The inverse function theorem gives a sufficient condition for the invertibility of a function near some point in its domain. If the derivative $f'$ of a function $f$ is continuous and non-zero near some point $a$ within its domain, then the function is invertible near that point. If $b = f(a)$, then:

\begin{equation}
\frac{d\left[f^{-1}(b)\right]}{dx} = \frac{1}{\frac{df(a)}{dx}}
\end{equation}

That is, the derivative of the inverse function at a point $b=f(a)$ of the range, is the reciprocal of the derivative of the function near the point $a$ in the domain. This extends to multivariable calculus. Given a function $\mathbf{f}: \mathbf{x} \rightarrow \mathbf{y}$:

\begin{equation}
\nabla_\mathbf{y}\left[\mathbf{f}^{-1}\right] = \left[\nabla_\mathbf{x} \mathbf{f} \right]^{-1}
\end{equation}

In words: the Jacobian of the inverse function at the point $\mathbf{b} = \mathbf{f}(\mathbf{a})$ is the matrix inverse of the Jacobian of the function at the point $\mathbf{a}$. The sufficient condition is that the Jacobian $\nabla_\mathbf{x}\mathbf{f}$ is continuous and \textit{nonsingular} near $\mathbf{a}$.

\subsection{Critical Points}
Critical points are points where the Jacobian does not have maximal rank. In case of a square Jacobian, this means that the Jacobian is singular. 

\subsection{Differential Volume Element / Change of Variables}
The Jacobian is used when transforming between different coordinate systems. Consider a transformation $\mathbf{x} = \mathbf{H}(\mathbf{y})$, then:

\begin{equation}
\mathrm{d}^n x = \left|\nabla_\mathrm{y} \mathbf{H} \right| \mathrm{d}^n y
\end{equation}

And:

\begin{equation}
\int_\mathbf{x} \mathrm{d}^n\mathbf{x} f(\mathbf{x}) = \int_\mathbf{y} \mathrm{d}^n\mathbf{y} \left|\nabla_\mathrm{y} \mathbf{H} \right| f(\mathbf{H}(\mathbf{y}))
\end{equation}

Alternatively, if $\mathbf{y} = \mathbf{H}^{-1}(\mathbf{x})$:

 \begin{equation}
 \begin{array}{rl}
 \mathrm{d}^n y &= \left|\nabla_\mathrm{x} \mathbf{H}^{-1}(\mathbf{x})\right| \mathrm{d}^n x\\
 &= \left| \left[ \nabla_\mathrm{y} \mathbf{H}(\mathbf{y}) \right]^{-1} \right| \mathrm{d}^n x\\
 \mathrm{d}^n x &= \frac{1}{\left| \left[ \nabla_\mathrm{y} \mathbf{H}(\mathbf{y}) \right]^{-1} \right|}  \mathrm{d}^n y 
 \end{array}
 \end{equation}

The Jacobian has to be nonsingular within the domain of integration. This implies that $\mathbf{x}$ and $\mathbf{y}$ have to have the same dimension. In the context of probability theory that sometimes requires artificially defining additional variables so that $\mathbf{H}$ is bijective because the quantity of interest has lower dimension (for example, if you calculate the mean of a random variable). 

\subsection{Hessian}

The Hessian is the second derivative of a scalar valued function $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ with respect to a vector, i.e. $\nabla\cdot\nabla f$. The elements are $\mathbf{H}(f)_{i,j} = \frac{\partial^2 f}{\partial x_i\partial x_j}$. 

\begin{equation}
\mathbf{H}(f) = \left[\begin{array}{cccc}
\frac{\partial^2f}{\partial x_1^2}&\frac{\partial^2f}{\partial x_1\partial x_2} & \hdots & \frac{\partial^2f}{\partial x_1\partial x_n}\\
\frac{\partial^2f}{\partial x_2\partial x_1}&\frac{\partial^2f}{\partial x_2^2} & \hdots & \frac{\partial^2f}{\partial x_1\partial x_n}\\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2f}{\partial x_n\partial x_1}&\frac{\partial^2f}{\partial x_n\partial x_2} & \hdots & \frac{\partial^2f}{\partial x_n^2}
\end{array} \right]
\end{equation}

The Hessian of a vector valued function $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ is a third order tensor with elements  $\mathbf{H}(\mathbf{f})_{i,j,k} = \frac{\partial^2 f_k}{\partial x_i\partial x_j}$.