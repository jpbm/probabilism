\section{Point Mass Distributions}

If the entire probability is concentrated at a single point $\alpha$, then that can be expressed in terms of the Kronecker Delta in the discrete case and the Dirac Delta function in the continuous case.

\subsection{Kronecker Delta $\delta_{\alpha}$}
In the discrete case:

\begin{equation}
\delta_{\alpha} = \left\{\begin{array}{l} 1\mathrm{\ if\ }x = \alpha \\ 0\mathrm{\ else}\end{array} \right.
\end{equation}

\subsection{Dirac Delta Function $\delta(x-\alpha)$}
In the continuous case:

\begin{equation}
\int_A \delta(x-\alpha) \mathrm{d}x = \left\{\begin{array}{l} 1\mathrm{\ if\ }\alpha \in A \\ 0\mathrm{\ else}\end{array} \right.
\end{equation}

The Dirac Delta function is actually a distribution with extremely useful properties that I ought to write up. 

\section{Uniform Distributions}

\subsection{Discrete Uniform $\mathrm{Uniform(1,k)}$}
With $k>0$ be some integer, the PMF is:

\begin{equation}
f(x) = \left\{\begin{array}{l} 1/k \mathrm{\ if\ }x \in \{1,...,k\} \\ 0\mathrm{\ else}\end{array} \right.
\end{equation}

The CDF is:

\begin{equation}
F(x)= \left\{\begin{array}{cl} 
0& x < 1\\
\frac{x}{k}& 1\leq x \leq k\\
1& x>k
\end{array} \right.
\end{equation}

% continuous uniform
\subsection{Continuous Uniform $\mathrm{Uniform(a,b)}$}

With $a,b\in\mathbb{R}$ and $b>a$, the PDF is:

\begin{equation}
f(x) = \left\{\begin{array}{l} 1/(b-a) \mathrm{\ if\ }x \in [a,b] \\ 0\mathrm{\ else}\end{array} \right.
\end{equation}

The CDF is:

\begin{equation}
F(x)= \left\{\begin{array}{cl} 
0& x < a\\
\frac{x-a}{b-a}& a\leq x \leq b\\
1& x>b
\end{array} \right.
\end{equation}


% bernoulli 
\section{Bernoulli Processes}
A Bernoulli process is a number of discrete trials with binary outcome, for example a series of coinflips. This is typically framed in terms of success $X=1$ with probability $p$ and failure $X=0$ with probability $1-p$. The trials are independent from each other. Thinking of the process as a sequence in time, this means that the process is memoryless: the number of successes or time since the last success have no bearing on the future. Thinking about the process as a sequence in space, the successes are independently scattered over a grid like randomly flipped bits in a string of bits. That is, the events are uniformly distributed.


\subsection{Bernoulli $\mathrm{Bernoulli(p)}$}
The PMF of a single binary outcome $X=1$ with probability $p$ and $X=0$ with probability $1-p$. The PMF is:

\begin{equation}
f(x) = p^x (1-p)^{1-x} =  \left\{\begin{array}{cl} 
p& x = 1\\
(1-p)& x=0
\end{array} \right.
\end{equation}

For $x\in\{0,1\}$ and $p\in[0,1]$.


% binomial
\subsection{Binomial $\mathrm{Binomial}(n,p)$}
The binomial distribution is the PMF for the number of successes with probability $p$ among $n$ trials. That is, if $X_i \sim \mathrm{Bernoulli}(p)$, then $X = \sum_i^n X_i$ has distribution:

\begin{equation}
f(x) = {n \choose x} p^x (1-p)^{n-x}
\end{equation}

It follows that if $X_1 \sim \mathrm{Binomial}(n_1,p)$ and $X_2 \sim \mathrm{Binomial}(n_2,p)$, then $X_1 + X_2 \sim \mathrm{Binomial}(n_1+n_2,p)$. The binomial distribution can be interpreted of the probability that there will be $x$ successes among $n$ draws with replacement.


% geometric distribution
\subsection{Geometric $\mathrm{Geom}(p)$}
The PMF for the number of Bernoulli trials with parameter $p\in(0,1)$ is given by: 

\begin{equation}
f(x) = p(1-p)^{x-1}
\end{equation}

In the time picture, it models the number of intervals $x$ until the first success occurs, or, equivalently, the number of trials between two successes. In the space picture, it models the distance between successes that are independently scattered on a grid.



% negative binomial distribution
\subsection{Pascal, Negative Binomial $\mathrm{NB}(r,p)$}
The negative binomial distribution with parameters $r$ and $p$ gives the sum of $r$ geometric random variables with parameter $p$. In the time picture, if $X \sim \mathrm{NB}(r,p)$, $X$ gives the probability for the number of failures in a sequence of Bernoulli trials until there are $r$ successes. In the space picture, it the probability for the width of the interval of a grid between $r$ successes (not counting the spots taken up by the successes). Its PMF is given by:

\begin{equation}
f(x) = {x+r-1\choose x} p^r (1-p)^x
\end{equation} 

For $r=1$, the distribution is the same as the geometric distribution with parameter $p\rightarrow 1-p$. The negative binomial distribution gives the probability that, when drawing with replacement, it will take $x$ failures until there have been $r$ successes, where each success has probability $p$.

% bernoulli without replacement
\section{Bernoulli Processes 	Without Replacement}
Bernoulli Processes were a sequence of independent trials, which can be thought of as a sequence of samples with replacement. The "hyper"- distributions treat the analogous case where samples are not replacemed.  

% hypergeometric distribution
\subsection{Hypergeometric $\mathrm{Hypergeom(N,K,n)}$}
The hypergeometric distribution gives the probability for the number of successes when drawing $n$ times from a population of $N$, of which $K$ correspond to successes.

\begin{equation}
f(k) = \frac{{K \choose k}{ N-K \choose n-k}}{{N \choose k}}
\end{equation} 

It is the analogue to the binomial distribution for sampling without replacement.

% negative hypergeometric distribution
\subsection{Negative Hypergeometric $\mathrm{NH(N,K,n)}$}
The negative hypergeometric distribution gives the probability that, when sampling without replacement, it will take $r$ failures until there have been $k$ successes, if the total population is $N$ and the number of elements corresponding to a success is $K$.

\begin{equation}
f(k) = \frac{{k+r-1\choose k}{N-r-k \choose K-k}}{{N\choose K}}
\end{equation}

The negative hypergeometric distribution is the analogue to the negative binomial distribution for sampling without replacement. 


% poisson processes
\section{Poisson Point Processes}
Poisson point processes are the continuous analogue to Bernoulli processes. Rather than looking at the outcome of a number of binary trials, they look at the number or spacing of independent point events over a continuous interval. In one dimension, Poisson Processes very often model rare events happening over a given time interval. In higher dimensions, the Poisson process can be thought of as independently scattered points in some volume. The process is typically characterized by the parameter $\Lambda$, called the \textit{rate} or \textit{intensity} of the process. It can be written $\Lambda = \nu \lambda$ where $\nu$ is a Lebesgue measure (assigning a length or volume to a set) and $\lambda$ is a constant. For a temporal process, $\nu$ would be the time interval. For a spatial process, $\nu$ would be a volume. The events have uniform distribution over the measure. This is also called a homogenous Poisson process. 

% poisson distribution
\subsection{Poisson $\mathrm{Poisson}(\Lambda)$}
The poisson distribution is the continuous analogue to the Binomial distribution. It measures the number of events for a process with intensity $\Lambda$.

\begin{equation}
f(x) = e^{-\Lambda}\frac{\Lambda^x}{x!}\ \ \mathrm{x\geq0}
\end{equation}

Just like for the Binomial distribution, if $X_1 \sim \mathrm{Poisson}(\Lambda_1)$ and $X_2 \sim \mathrm{Poisson}(\Lambda_2)$, then $X_1 + X_2 \sim \mathrm{Poisson}(\Lambda_1 + \Lambda_2)$. 


% exponential distribution
\subsection{Exponential $\mathrm{Exp}(\lambda)$}
The exponential distribution is the continuous analogue to the Geometric distribution. In one dimension, if $X \sim \mathrm{Exp}(\Lambda)$, then $X$ can be interpreted as the distance between two independently scattered points on the real line. In the time picture, that would be the time that elapses between two events, or, equivalently, the time until the next event, where $\lambda$ is the rate or intensity.

\begin{equation}
f(x) = \lambda e^{-\lambda}
\end{equation}


% gamma distribution
\subsection{Gamma $\mathrm{Gamma(\alpha,\beta)}$}
The Gamma Distribution is the continuous analogue to the negative binomial distribution. For integer $\alpha$, it gives the probability of the value of the sum of $\alpha$ exponentially distributed random variables with parameter $\beta=\frac{1}{\lambda}$. That is, if $X_i \sim Exp(\frac{1}{\beta})$ then $X = \sum_i^\alpha X_i \sim \mathrm{Gamma}(\alpha,\beta)$. The PDF is given by:

\begin{equation}
f_x = \frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}
\end{equation}

With $\alpha,\beta > 0$.

As one would expect, if $X_1 \sim \Gamma(\alpha_1,\beta)$ and $X_2 \sim \Gamma(\alpha_2,\beta)$ then $X_1 + X_2 \sim \Gamma(\alpha_1+\alpha_2,\beta)$. 



% t distribution
\section{$t$-Distribution $\mathrm{t_\nu\ }$}

\subsection{Cauchy Distribution $t_1$}
The Cauchy Distribution does not have a mean. It is infinite.


\section{Chi$^2$-Distribution $\chi^2_p$}
The $\chi^2$ distribution is the probability distribution of the sum of squares of standard normally distributed random variables. If $Z_i$ has standard normal distribution, then $X = \sum_i^p Z_i \sim \chi^2_p$.



\section{Normal $\mathscr{N}(\mu,\sigma^2)$}


\subsection{Standard Normal Distribution $\mathscr{N}(0,1)$ }

The normal distribution is so ubiquitous that there is notation specifically for the \textit{standard normal distribution}, which is the normal distribution with parameters $\mu = 0$ and $\sigma = 1$. It is always possible to transform a random variable $X$ to have standard normal distribution by performing a coordinate transform. The PDF of the standard normal distribution is written $\phi(x)$ and the CDF $\Phi(x)$. The PDF is given by: 

\begin{equation}
\phi(x) = \frac{1}{\sqrt{2\pi}}\exp{-x^2}
\end{equation}

The CDF has no closed form expression. 

By convention, $Z$ is the random variable that has standard normal distribution. If $X \sim \mathscr{N}(\mu,\sigma^2)$, then $Z = \frac{X-\mu}{\sigma} \sim \mathscr{N}(0,1)$. This standardizing transformation allows for calculations using lookup tables. 

\subsubsection{Example: Interval $\mathbb{P}(a<X<b)$}

\begin{equation}
\mathbb{P}(a<X<b) =  \mathbb{P}(a' < Z < b') = \Phi(b') - \Phi(a')
\end{equation}

with $a' = \frac{a-\mu}{\sigma}$ and $b' = \frac{b-\mu}{\sigma}$.

\subsubsection{Example: Quantile $x = F^{-1}(q)$}

\begin{equation}
z = \Phi^{-1}(q)
\end{equation}

with $z = \frac{x-\mu}{\sigma}$, so that $x = \sigma \Phi^{-1}(q)+\mu$. The interquartile range is calculated by transforming $(z_{1}=\Phi^{-1}(0.25),z_{3}=\Phi^{-1}(0.75))$, and so on.


% Multinoulli Processes
\section{Categorial Processes, Multinoulli Processes}
A Multnoulli process is a number of discrete trials that may assume a number of different categorical outcomes, for example a series of dice throws or a random sequence of letters. It is the generalization of the Bernoulli process to processes with more than two possible outcomes. The trials are independent from each other. Thinking of the process as a sequence in time, this means that the process is memoryless: the sequence of future outcomes is independent of past outcomes. Thinking about the process as a sequence in space, it is a uniformly random assignment of one of a set number of outcomes to grid points.

% Multinoulli
\subsection{Categorical, Multinoulli $\mathrm{Categorical(\mathbf{p})}$}
The categorial or Multinoulli distribution is the generalization of the Bernoulli distribution to more than two possible outcomes. The PMF is given by:

\begin{equation}
f(\mathbf{x}) = \prod_i^k p_i^{[x=k]}
\end{equation}

With $p_i \in \mathbf{p}$ a discrete probability distribution and $[x=k]$ is the Iverson bracket.

% Multinomial
\subsection{Multinomial $\mathrm{Multinomial(n,\mathbf{p})}$}
The multinomial distribution is the multivariate generalization of the binomial process. Rather than a binary outcome, there are $k$ possible outcomes. If $\mathbf{X}=(X_1,X_2,...,X_k)$ has multinomial distribution with parameter vector $\mathbf{p} = (p_1,p_2,...,p_k)$, then it gives the probability that out of $n$ trials, $x_j$ will be of type $j$, where the probabilities of the different classes is given by the discrete probability distribution $\mathbf{p}$.

\begin{equation}
f(\mathbf{x}) = {n \choose x_1,x_2,...,x_k} p_1^{x_1}p_2^{x_2}...p_k^{x_k} = {|\mathbf{x}|\choose \mathbf{x}}\mathbf{p}^{\mathbf{x}}
\end{equation}

Where the final expression uses multi-index notation. The binomial process corresponds to $k=2$ classes (success and failure). The marginal distribution of $X_j$ is $Binomial(n,p_j)$.


% beta distribution
\section{Beta $\mathrm{Beta}(\alpha,\beta)$}

The PDF for the beta distribution is:

\begin{equation}
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\ \ 0 < x < 1
\end{equation}

With $\alpha, \beta > 0$.

The Beta distribution is the conjugate prior to the Bernoulli and Binomial distributions.

% dirichlet distribution
\section{Dirichlet $\mathrm{Dirichlet}(\mathbf{\alpha})$}
The Dirichlet Distribution is the multivariate generalization of the Beta Distribution. Given $\mathbf{X} = (X_1,...,X_K)$ and the parameter vector $\mathbf{\alpha} = (\alpha_1,...,\alpha_K),\ \alpha_j > 0$, the PDF is:

\begin{equation}
f(\mathbf{x},\mathbf{\alpha}) = \frac{1}{B(\mathbf{\alpha})} \prod^K_{i=1} x_i^{\alpha_i -1}
\end{equation}

Where $B(\mathbf{\alpha})$ is the multivariate Beta function:

\begin{equation}
B(\mathbf{\alpha}) = \frac{\prod^K_{i=1}\Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^K\alpha_i)}
\end{equation}

The Dirichlet distribution is the conjugate prior to the Multinoulli/Categorical and Multinomial distributions. The marginal distributions are Beta distributions, i.e. $X_i \sim \mathbf{Beta}(\alpha_i,\alpha_0-\alpha_1)$ where $\alpha_0 = \sum_j \alpha_j$.



% multivariate normal distribution
\section{Multivariate Normal $\mathscr{N}(\mathbf{\mu},\mathbf{\Sigma})$}
The multivariate normal distribution is the multivariate generalization of the normal distribution. The PDF is given by:

\begin{equation}
f{\mathbf{x}} = \frac{1}{(2\pi)^{\frac{k}{2}}|\Sigma|^{\frac{1}{2}}}\exp\{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T \mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\}
\end{equation}



% covariance matrix
\subsection{Covariance Matrix $\mathbf{\Sigma}$}
The covariance matrix $\mathbf{\Sigma}$ is symmetric and positive definite. Therefore, $\mathbf{\Sigma}^{\frac{1}{2}}$ exists and is real, symmetric, and

\begin{itemize}
\item $\mathbf{\Sigma}^{\frac{1}{2}}\mathbf{\Sigma}^{\frac{1}{2}} = \mathbf{\Sigma}$
\item $\mathbf{\Sigma}^{\frac{1}{2}}\mathbf{\Sigma}^{-\frac{1}{2}}=\mathbf{\Sigma}^{-\frac{1}{2}}\mathbf{\Sigma}^{\frac{1}{2}}=\mathbf{I}$
\end{itemize}


% marginal distribution 
\subsection{Marginal Distribution}
If $\mathbf{X} \sim \mathscr{N}(\mathbf{0},\mathbf{\Sigma})$, then the marginal distribution of $X_a$ is:

\begin{equation}
X_a \sim \mathscr{\mu_a,\Sigma_{aa}}
\end{equation}


% conditional distribution
\subsection{Conditional Distribution}
If $\mathbf{X} \sim \mathscr{N}(\mathbf{0},\mathbf{\Sigma})$, then the conditional distribution of $X_b$ given $X_a = x_a$ is:

\begin{equation}
X_b|X_a = x_a \sim \mathscr{N}(\mu_b + \Sigma_{ba}\Sigma_{aa}^{-1}(x_a - \mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}) 
\end{equation}


% vector multiplication 
\subsection{Vector Multiplication}
If $\mathbf{a}$ is a vector, then:

\begin{equation}
\mathbf{a}^T \mathbf{X} \sim \mathscr{N}(\mathbf{a}^T \mathbf{\mu}, \mathbf{a}^T\mathbf{\Sigma}\mathbf{a})
\end{equation}


% relationship to chi2
\subsection{Relationship to Chi$^2$}
\begin{equation}
V = \mathbf{Z}^T\mathbf{Z} = (\mathbf{X}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{X}-\mathbf{\mu}) \sim \chi^2_k
\end{equation}

\subsection{Multivariate Standard Normal Distribution $\mathscr{N}(\mathbf{0},\mathbf{\Sigma})$}
The standard multivariate normal distribution for $\mathbf{Z} = (Z_1,...,Z_k)$ with $\mathbf{Z} \sim \mathscr{N}(\mathbf{0},\mathbf{I})$ is:

\begin{equation}
f(z) = \prod^k_{i=1} f(z_i) = \frac{1}{(2\pi)^{\frac{k}{2}}}\exp \{ \frac{1}{2} \sum^{k}_{j=1} z_j\} = \frac{1}{(2\pi)^{\frac{k}{2}}}\exp \{ \frac{1}{2} \mathbf{z^T z}\}
\end{equation}

The transformation to a general multivariate normal random variable $\mathbf{X}$ is that if $\mathbf{Z} \sim \mathscr{0,\mathbf{I}}$, then $\mathbf{X} = \mathbf{\mu} + \mathbf{\Sigma}^{\frac{1}{2}}\mathbf{Z}$. Conversely, if $\mathbf{X}\sim \mathscr{\mathbf{\mu},\mathbf{\Sigma}}$ then $\mathbf{\Sigma}^{-\frac{1}{2}}(\mathbf{X}-\mathbf{\mu}) \sim \mathscr{N}(0,\mathbf{I})$.
