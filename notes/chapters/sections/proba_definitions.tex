\section{Interpretations and Definitions of Probability}

\subsection{Naive and Non-Naive Definitions of Probability}
Probability courses typically start with what \citeasnoun{blitzstein2019introduction} calls the \textit{naive} definition of probability, which is to look at the fraction of the event space that corresponds to a particular event. This works well for combinatorial type of probability problems, such as the likely outcome of dice throws. The dichotomy below is from \citeasnoun{blitzstein2019introduction}.

\subsubsection{Naive Probability}
The event space $S$ consists of a collection of equally likely outcomes. The actual outcome $s_{actual} \in S$. The probability of an outcome in a subset $A\subseteq S$, $P(s_{actual}\in A)$ corresponds to the fraction of events in $A$ out of $S$.

\begin{equation}
P_{naive}(A) = \frac{|A|}{|S|} = \frac{\mathrm{number of outcomes favorable to A}}{\mathrm{total number of outcomes in S}}
\end{equation}

This approach runs into trouble when the outcomes in $|A|$ are not equally likely, or if the sample space is infinite, i.e. $|S|=\infty$.


\subsubsection{Non-Naive Definition of Probability}
A probability space consists of a sample space $S$ in addition to a \textit{probability function}. The job of the probability function is to take an event $A\subseteq S$ and map it to a number between $0$ and $1$. I.e.  $P: S \rightarrow [0,1]$.

The function $P$ must satisfy:

\begin{itemize}
\item P(\empty) = 0, P(S) = 1
\item If $A_1, A_2,...$ are \textit{disjoint} events, then: \begin{equation}P\left(\bigcup^{\infty}_{j=1}A_j \right) = \sum_{j=1}^{\infty}P(A_j)\end{equation}
\end{itemize}

In other words, all you need to work with probability is a set of possible outcomes $S$ and some function to apply to parts of that set.

\subsection{Frequentist Probability}
The \textit{frequentist} interpretation of probability is that it represents the frequency of an outcome of a particular experiment upon running the experiment many times. On one hand, this view on probability is arguably empirically grounded. On the other hand it relies on the impractical notion of identically repeating something a large number of times. 

\subsection{Bayesian Probability}
The \textit{Bayesian} view on probability is that it represents a degree of belief in a particular outcome. On one hand, this implies a degree of subjectivity. On the other hand, this notion of probability is much more broadly applicable, because it does not require repeatable experiments. It also permits the introduction of subjective biases through priors.

I'm not sure whether the supposed "battle" between frequentists and Bayesians has ever been of any consequence for me. Also, the frequentist perspective strikes me as inherently contradictory, because supposed empirical proof can strictly speaking never be practically delivered.

In practice, it seems that frequentist approaches tend to be structured around parametrization of solutions in terms of summary statistics, while Bayesians have a tendency to approach problems in terms of full probability distributions. Social scientists seem to be leaning towards frequentist methods, while physicists and engineers seem to be more interested in Bayesian data analysis. 

I suspect that an additional source of bias is that frequentist methods tend to be computationally lighter and that many standard accepted recipes exist for common problems. In contrast, Bayesian approaches are usually derived ad-hoc in the context of a particular problem.

\subsection{Measure Theoretic Probability}
The modern approach to probability is rooted in measure theory. In that context:

\being{itemize}
\item The \textit{sample space} is a measurable set
\item An \textit{event} is a measurable set; a subset of the sample space.
\item A \texit{random variable} is a measurable function on the sample space.
\item The \textit{expectation} of a random variable is its integral with respect to the probability measure.
\end{itemize}


