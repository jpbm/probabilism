\section{Types of Matrices, Matrix Properties}

\subsection{$\mathrm{sgn}\left(\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x}\right)$ Definite}
\label{sec:definite}

A hermitian matrix $\mathbf{H}\in\mathbb{C}^n$ is positive definite, if for any non-zero column vector $\mathbf{x}\in\mathbb{C}^n$, the quadratic form $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} > 0$, and negative definite if $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} < 0$. The matrix is positive or negative \textit{semidefinite} if  $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} \geq 0$ or $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} \leq 0$, respectively. Definiteness plays a role in investigating the convexity of a function by looking at the Hessian (section \ref{sec:hessian}). Sometimes notation with curly comparison symbols are used. The relationship between definiteness and eigenvalues is intuitive:

\begin{tabular}{lll}
$\mathbf{H} \succ 0$ & positive definite & all eigenvalues are positive\\
$\mathbf{H} \prec 0$ & negative definite & all eigenvalues are negative\\
$\mathbf{H} \succeq 0$ & positive semidefinite & all eigenvalues are positive or 0\\
$\mathbf{H} \preceq 0$ & negative semidefinite & all eigenvalues are negative or 0
\centering
\end{tabular}

The curly comparison symbols can mean other stuff though, for example in the context of partially ordered sets (order theory) or comparisons between multidimensional arrays.





% unitary
\subsection{Triangular}
\label{sec:triangular}
A lower triangular matrix is a matrix that has all-zero entries above the diagonal.


\begin{equation}
\mathbf{L} = \left[\begin{array}{cccccc} l_{1,1}&&&&&0\\l_{2,1}&l_{2,2}&&&&\\l_{3,1}&l_{3,2}&\ddots&&&\\  \vdots&\vdots&\ddots&\ddots&&\\ \vdots&\vdots&&\ddots&\ddots&\\  l_{n,1}&l_{n,2}&\hdots&\hdots&l_{n,n-1}&l_{n,n}\end{array}\right]
\end{equation}

Upper triangular matrices are matrices that have all-zero entries below the diagonal.



% commuting 
\subsection{$\mathbf{AB}-\mathbf{BA}=0$ Commuting}
Two matrices commute if $\mathbf{AB}=\mathbf{BA}$, or, equivalently, their \textit{commutator} $[\mathbf{A,B}] = \mathbf{AB}-\mathbf{BA}$ is zero. This means that $\mathbf{A}$ and $\mathbf{B}$ both have to be square. Matrices commute when they have the same eigenspace, i.e. they have the same eigenvectors. This can be seen by considering the diagonal representations of $\mathbf{A}$ and $\mathbf{B}$.

Let $\mathbf{A}$ and $\mathbf{B}$ be two square matrices with the same eigenvectors $\mathbf{V}$, then they can be diagonalized as:

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda_A V^{-1}}\\
\mathbf{B} = \mathbf{V\Lambda_B V^{-1}}
\end{equation}

They commute, because:

\begin{equation}
\mathbf{AB} = \mathbf{V\Lambda_A V^{-1}V\Lambda_B V^{-1}} = \mathbf{V\Lambda_B \Lambda_A V^{-1}} = \mathbf{V\Lambda_B V^{-1}V\Lambda_A V^{-1}} = \mathbf{BA} 
\end{equation}



% anticommuting
\subsection{$\mathbf{AB}+\mathbf{BA}=0$ Anticommuting}
Two matrices anticommute if $\mathbf{AB}=-\mathbf{BA}$, or, equivalently, their \textit{anticommutator} $\{\mathbf{A,B}\} = \mathbf{AB}+\mathbf{BA}$ is zero.


% Hermitian / Symmetric
\subsection{$\mathbf{A}^{\dagger} = \mathbf{A}$ Hermitian, Symmetric}
\label{sec:hermitian}
Hermitian matrices are matrices that are equal to their complex transpose. That is:

\begin{equation}
{\mathbf{A}^{*}}^T = \mathbf{A}^\dagger = \mathbf{A} 
\end{equation}

\subsubsection{Properties}
(There are more)
\begin{itemize}
\item By definition: $\mathbf{A} = \mathbf{A}^\dagger$
\item Diagonal Entries are all real, since $a_{i,i} = a_{i,i}^*$, but not necessarily positive (physics will mislead you there...)
\item Inverse is also hermitian:  $\mathbf{A}^{-1} ={ \mathbf{A}^{-1}}^\dagger$
\item Diagonalizable with real eigenvalues and orthogonal eigenvectors $\in \mathbb{C}^n$.
\end{itemize}


Hermitian matrices with only real entries are called symmetric matrices. In that case $\mathbf{A}^T = \mathbf{A}$.

Hermitian matrices can only have real elements along their diagonal. 



% Skew Hermitian / Skew Symmetric
\subsection{$\mathbf{A}^{\dagger} = -\mathbf{A}$ Skew Hermitian, Skew Symmetric}
Skew Hermitian matrices that are equal to the negative of their complex transpose. That is:

\begin{equation}
{\mathbf{A}^{*}}^T = \mathbf{A}^\dagger = -\mathbf{A} 
\end{equation}

Real matrices that are skew Hermitian are called skew symmetric. In that case:

\begin{equation}
\mathbf{A}^T = -\mathbf{A} 
\end{equation}


Skew Hermitian matrices can only have complex values on their diagonal, and skew symmetric matrices can only have zeros as diagonal elements.




% Involutory
\subsection{$\mathbf{A}\mathbf{A}=\mathbf{I}$ Involutory}
Involutory matrixes are matrices that are their own inverse, so that:

\begin{equation}
\mathbf{A}\mathbf{A}=\mathbf{I}
\end{equation}

Involutory matrices are all square roots of the identity matrix. A famous example are the $2\times 2$ Pauli matrices. 




% Isometric
\subsection{$||\mathbf{A}\mathbf{x}||_{\alpha} = ||\mathbf{x}||_{\alpha}$ Isometric}
\label{sec:isometric}
An isometric transformation with respect to some norm $||\cdot||_{\alpha}$ preserves that norm (it's in the name: iso-metric). The linear case is represented by isometric matrices, which satisfy:

\begin{equation}
||\mathbf{A}\mathbf{x}||_{\alpha} = ||\mathbf{x}||_{\alpha} 
\end{equation}

For some vector $\mathbf{x}$. Isometries are also known as distance-preserving maps. To see this, define the distance between two points $\mathbf{a}$ and $\mathbf{b}$ as $||\mathbf{a} - \mathbf{b}||_{\alpha} = ||\mathbf{x}||_{\alpha}$. Isometries are usually understood be bijective. 

In terms of operator norms, isometries must satisfy $||\mathbf{A}||_{(\alpha)} = 1$ where $||\cdot||_{(\alpha)}$ denotes the operator norm (cf. section \ref{sec:operatornorm}). However, operator norms give the upper bound on the distortion of the input, so a unit operator norm is necessary but not sufficient. 


\subsubsection{Isometries with Respect to $L^2$}
The $L^2$ norm is unique in that the points $\{\mathbf{x}: ||\mathbf{x}||_{2} = 1\}$ lie on a perfect circle, which remains a circle regardless of the orientation of the underlying coordinate system. (A sphere looks the same regardless of angle.) This symmetry is broken for all other norms. Unitary matrices are isometries with respect to $L^2$, though unitarity is not a necessary condition for an isometry.

\subsubsection{Isometries with Respect to $L^1$ and general $L^q\neq L^2$}
Isometries with respect to $L^1$ are relevant because they are permissible transformations of (classical) probability distributions. For example, the transition matrix that describes the flow of probability between the time steps of a Markov Chain has to ensure that the entries of the state space probability still sum to $1$. 

For norms $L^q$ with $q\neq2$, the unit "circle" $\{\mathbf{x}: ||\mathbf{x}||_{q\neq2} = 1\}$ is not perfectly round. Instead the symmetry is broken along the coordinate axes. That means that a rotation of the coordinate axes gives a different unit distance, and so two points $\mathbf{a}$ and $\mathbf{b}$ that are $||\mathbf{a}-\mathbf{b}||_{q\neq2} = 1$ apart in one coordinate system may have a different separation in some other coordinate system. 

In particular, for the $L^1$, or \textit{Manhattan} norm (cf. section \ref{sec:l1norm}), the points $\{\mathbf{x}: ||\mathbf{x}||_{1} = 1\}$ lie on a diamond with axes aligned to the axes of the coordinate system. In Manhattan, reaching a point that is $1$ mile away "as the crow flies", depends on the position of that point with respect to the grid of streets and avenues. When that grid is rotated, the point may be quicker or take longer to reach. 

Since this rules out rotations, the internet tells me that the only linear maps that are isometries for $L^q$ with $q\neq 2$ are signed permutation matrices. That is, matrices that assign $\hat{x}\rightarrow \hat{y}$ or $\hat{z} \rightarrow -\hat{x}$ and so on. 

However, in general, any transformation that maps a point on the $L^q$ unit circle to another point on the $L^q$ unit circle is an isometry with respect to $L^q$, and this class of transformation is more general than signed permutations. The stochastic matrices are an example with respect to $L^1$.

\subsection{Stochastic}

Stochastic matrices are used to describe transitions between states of a Markov chain. The matrices are square and each entry is non-negative and represents a conditional probability of moving from one state to another. The entries along either the row or the column, or both, must sum to 1 according to the requirement of marginalization.

\begin{itemize}
\item Right stochastic has rows that sum to 1, so that $\mathbf{P}\mathbf{1}=\mathbf{1}$. It is applied $\mathbf{\pi}\mathbf{P}$. Column index gives "from", row index gives "to". 
\item Left stochastic has columns that sum to 1, so that $\mathbf{1}\mathbf{P}=\mathbf{1}$. It is applied $\mathbf{P}\mathbf{\pi}$. Row index gives "from", column index gives "to".
\item doubly stochastic has rows and columns that sum to 1. 
\end{itemize}

Products of stochastic matrices are also stochastic matrices. The spectral radius (largest eigenvalue) of a stochastic matrix is always $1$. Since $\mathbf{1}$ is an eigenvector and the left and right eigenvalues of a square matrix are the same, there is at least one stationary state $\mathbf{\pi}\mathbf{P}=\mathbf{\pi}$ (that is, in this case of a right stochastic matrix, a left eigenvector) with eigenvalue $1$. 



% Unitary / Orthogonal
\subsection{$\mathbf{U}^{\dagger} = \mathbf{U}^{-1}$ Unitary, Orthogonal}
\label{sec:unitary}
Unitary matrices satisfy $\mathbf{U}^{\dagger}\mathbf{U} = \mathbf{UU}^{\dagger}=\mathbf{I}$, and they have $det(\mathbf{U}) = 1$. They are diagonalizable and can be expressed as $e^{i\mathbf{H}}$ where $\mathbf{H}$ is a Hermitian matrix.

Unitary matrices that are real are called orthogonal.  Orthogonal matrices satisfy $\mathbf{A}^{-1} = \mathbf{A}^T$. The rows (and columns) of $\mathbf{A}$ are an orthonormal basis in $\mathrm{R}^n$.

Unitary matrices are necessarily invertible, and have determinant $|U|=1$ or $|U|=-1$. They represent \textit{unitary transformations}, which means that they preserve the inner product between two vectors.

The set of $n \times n$ orthogonal matrices is known as the orthogonal group $O(n)$ and the subgroup of orthogonal matrices with determinant $1$ is known as the special orthogonal group $SO(n)$. The elements of $SO(n)$ are rotations, and the elements of $O(n)$ represent translations, reflections or rotations. Similarly, the group of $n \times n$ unitary matrices is the unitary group $U(n)$ and the subgroup of $U(n)$ that has determinant $1$ is the special unitary group $SU(n)$.

Unitary transformations preserve the $L^2$ norm of vectors.


% Similarity
\subsection{$\mathbf{A} = \mathbf{TBT^{-1}}$ Similarity}
\label{sec:similiarity}
Two matrices are said to be similar if they can be related through a similarity transformation  $\mathbf{A} = \mathbf{TBT^{-1}}$ where $\mathbf{T}$ is some nonsingular matrix (cf. section \ref{sec:similaritytrans}). An important example is that square matrices are similar to diagonal matrices, see section \ref{sec:diagonalization}. 
   

