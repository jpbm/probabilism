\section{Types of Matrices}

\subsection{$\mathrm{sgn}\left(\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x}\right)$ Definite}
\label{sec:definite}

A hermitian matrix $\mathbf{H}\in\mathbb{C}^n$ is positive definite, if for any non-zero column vector $\mathbf{x}\in\mathbb{C}^n$, the quadratic form $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} > 0$, and negative definite if $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} < 0$. The matrix is positive or negative \textit{semidefinite} if  $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} \geq 0$ or $\mathbf{x}^{\dagger}\mathbf{H}\mathbf{x} \leq 0$, respectively. Definiteness plays a role in investigating the convexity of a function by looking at the Hessian (section \ref{sec:hessian}). Sometimes notation with curly comparison symbols are used. The relationship between definiteness and eigenvalues is intuitive:

\begin{tabular}{lll}
$\mathbf{H} \succ 0$ & positive definite & all eigenvalues are positive\\
$\mathbf{H} \prec 0$ & negative definite & all eigenvalues are negative\\
$\mathbf{H} \succeq 0$ & positive semidefinite & all eigenvalues are positive or 0\\
$\mathbf{H} \preceq 0$ & negative semidefinite & all eigenvalues are negative or 0
\centering
\end{tabular}

The curly comparison symbols can mean other stuff though, for example in the context of partially ordered sets (order theory) or comparisons between multidimensional arrays.





% unitary
\subsection{Triangular}
\label{sec:triangular}
A lower triangular matrix is a matrix that has all-zero entries above the diagonal.


\begin{equation}
\mathbf{L} = \left[\begin{array}{cccccc} l_{1,1}&&&&&0\\l_{2,1}&l_{2,2}&&&&\\l_{3,1}&l_{3,2}&\ddots&&&\\  \vdots&\vdots&\ddots&\ddots&&\\ \vdots&\vdots&&\ddots&\ddots&\\  l_{n,1}&l_{n,2}&\hdots&\hdots&l_{n,n-1}&l_{n,n}\end{array}\right]
\end{equation}

Upper triangular matrices are matrices that have all-zero entries below the diagonal.



% commuting 
\subsection{$\mathbf{AB}-\mathbf{BA}=0$ Commuting}
Two matrices commute if $\mathbf{AB}=\mathbf{BA}$, or, equivalently, their \textit{commutator} $[\mathbf{A,B}] = \mathbf{AB}-\mathbf{BA}$ is zero. This means that $\mathbf{A}$ and $\mathbf{B}$ both have to be square. Matrices commute when they have the same eigenspace, i.e. they have the same eigenvectors. This can be seen by considering the diagonal representations of $\mathbf{A}$ and $\mathbf{B}$.

Let $\mathbf{A}$ and $\mathbf{B}$ be two square matrices with the same eigenvectors $\mathbf{V}$, then they can be diagonalized as:

\begin{equation}
\mathbf{A} = \mathbf{V\Lambda_A V^{-1}}\\
\mathbf{B} = \mathbf{V\Lambda_B V^{-1}}
\end{equation}

They commute, because:

\begin{equation}
\mathbf{AB} = \mathbf{V\Lambda_A V^{-1}V\Lambda_B V^{-1}} = \mathbf{V\Lambda_B \Lambda_A V^{-1}} = \mathbf{V\Lambda_B V^{-1}V\Lambda_A V^{-1}} = \mathbf{BA} 
\end{equation}



% anticommuting
\subsection{$\mathbf{AB}+\mathbf{BA}=0$ Anticommuting}
Two matrices anticommute if $\mathbf{AB}=-\mathbf{BA}$, or, equivalently, their \textit{anticommutator} $\{\mathbf{A,B}\} = \mathbf{AB}+\mathbf{BA}$ is zero.


% Hermitian / Symmetric
\subsection{$\mathbf{A}^{\dagger} = \mathbf{A}$ Hermitian, Symmetric}
\label{sec:hermitian}
Hermitian matrices are matrices that are equal to their complex transpose. That is:

\begin{equation}
{\mathbf{A}^{*}}^T = \mathbf{A}^\dagger = \mathbf{A} 
\end{equation}

\subsubsection{Properties}
There are many properties of Hermitian matrices. 
\begin{itemize}
\item By definition: $\mathbf{A} = \mathbf{A}^\dagger$
\item Diagonal Entries are all real, since $a_{i,i} = a_{i,i}^*$, but not necessarily positive (physics will mislead you there...)
\item Inverse is also hermitian:  $\mathbf{A}^{-1} ={ \mathbf{A}^{-1}}^\dagger$
\item Diagonalizable with real eigenvalues and orthogonal eigenvectors $\in \mathbb{C}^n$.
\end{itemize}


Hermitian matrices with only real entries are called symmetric matrices. In that case $\mathbf{A}^T = \mathbf{A}$.


% Skew Hermitian / Skew Symmetric
\subsection{$\mathbf{A}^{\dagger} = -\mathbf{A}$ Skew Hermitian, Skew Symmetric}
Skew Hermitian matrices that are equal to the negative of their complex transpose. That is:

\begin{equation}
{\mathbf{A}^{*}}^T = \mathbf{A}^\dagger = -\mathbf{A} 
\end{equation}

Real matrices that are skew Hermitian are called skew symmetric. In that case:

\begin{equation}
\mathbf{A}^T = -\mathbf{A} 
\end{equation}


% Unitary / Orthogonal
\subsection{$\mathbf{U}^{\dagger} = \mathbf{U}^{-1}$ Unitary, Orthogonal}
Unitary matrices satisfy $\mathbf{U}^{\dagger}\mathbf{U} = \mathbf{UU}^{\dagger}=\mathbf{I}$, and they have $det(\mathbf{U}) = 1$. They are diagonalizable and can be expressed as $e^{i\mathbf{H}}$ where $\mathbf{H}$ is a Hermitian matrix.

Unitary matrices that are real are called orthogonal.  Orthogonal matrices satisfy $\mathbf{A}^{-1} = \mathbf{A}^T$. The rows (and columns) of $\mathbf{A}$ are an orthonormal basis in $\mathrm{R}^n$.

Unitary matrices are necessarily invertible, and have determinant $|U|=1$ or $|U|=-1$. They represent unitary transformations, which means that they preserve the inner product between two vectors.

The set of $n \times n$ orthogonal matrices is known as the orthogonal group $O(n)$ and the subgroup of orthogonal matrices with determinant $1$ is known as the special orthogonal group $SO(n)$. The elements of $SO(n)$ are rotations, and the elements of $O(n)$ represent translations, reflections or rotations. Similarly, the group of $n \times n$ unitary matrices is the unitary group $U(n)$ and the subgroup of $U(n)$ that has determinant $1$ is the special unitary group $SU(n)$.



% Similarity
\subsection{$\mathbf{A} = \mathbf{TBT^{-1}}$ Similarity}
Two matrices are said to be similar if they can be related through a similarity transformation  $\mathbf{A} = \mathbf{TBT^{-1}}$ where $\mathbf{T}$ is some nonsingular matrix (cf. section \ref{sec:similaritytrans}). An important example is that square matrices are similar to diagonal matrices, see section \ref{sec:diagonalization}. 
   


% Involutory
\subsection{$\mathbf{A}\mathbf{A}=\mathbf{I}$ Involutory}
Involutory matrixes are matrices that are their own inverse, so that:

\begin{equation}
\mathbf{A}\mathbf{A}=\mathbf{I}
\end{equation}

Involutory matrices are all square roots of the identity matrix. A famous example are the $2\times 2$ Pauli matrices. 