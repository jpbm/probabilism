\label{Autoencoders}

Autoencoders are a dimensionality reduction technique that use neural networks to compress information in a lower-dimensional latent space. This is done by training a network that has a bottleneck, i.e. a layer with lower dimensionality, with the data used both as input and as output. The network up to the bottleneck ("code") is the encoder. The part of the network that takes the code as input is the decoder. 

In practice, classical autoencoders apparently don't generate particularly useful or nicely structured latent spaces, nor compress particularly well. 

I have frequently seen them used for anomaly detection: you train an autoencoder on your data. Datapoints that have particularly high error after passing through the autoencoder are considered outliers, i.e.  "anomalous". Call it part of the "reconstruction error" type of approach to anomaly detection.

In principle, autoencoders can be used as generational models by feeding values into the decoder. However, there is no guarantee that any particular region of the latent space encodes a meaningful representation. Variational Autoencoders address this to some extent.


\label{Variational Autoencoders}

Autoencoders are effectively functions that map a high-dimensional input vector to a fixed, low-dimensional code. The hope is that this possibly highly non-linear function correctly projects out some lower-rank structure in the input data. The range of this function is unknown, and picking a point in the co-domain, even if it is close to a point known to be in the range, will not necessarily decode to a meaningful datapoint. The connection between code and data is deterministic.

Variational Autoencoders address this by pretending that the data are manifestations of underlying stochastic processes. The "code" learned by the variational autoencoders are the latent parameters of the process that gave rise to the input. The "decoding" consists of plugging particular values for the latent parameters and sampling from the resulting distribution. The result of the sampling is compared to the input. The variational autoencoder learns latent parameters that improve the likelihood of the resulting distribution producing a sample similar to the input. Meaningful outputs that are similar to a particular input can be generated by sampling the distribution that uses as latent parameters the code of the input, or codes that are close to the code of the input. The randomness causes the latent space to be continuously meaningful. 

\begin{itemize}
\item An encoder takes input sample to two parameters in the latent space of representations: mean, variance
\item You sample a point from the latent normal distribution that's assumed to have generated the input via $z = \mu + \sigma \epsilon$ where $\epsilon \sim \mathcal{N}$. 
\item The decoder maps this point $z$ back to the input image. 
\end{itemize} 

\label{Concept Vectors}
Concept Vectors are directions in representation spaces that encode meaningful variations in the data. For example, in images of faces, there might be a "smile" vector, so that if $z$ is the latent-space coordinate of a face then $z+s$ migth be the latent-space coordinate of the same face smiling. 

The loss function for training VAEs has two parts: \textit{reconstruction loss} forces the output to be clsoe to the input, and \textit{regularization loss} forces structure upon the latent space (which also helps overfitting). In practice, \textit{regularization loss} might be the KL-divergence of the code distribution with respect to a standard Normal. 
