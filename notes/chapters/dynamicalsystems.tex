\chapter{Dynamical Systems}
\label{chp:dynamicalsystems}

\begin{multicols}{2}[\subsubsection*{Contents of this chapter}]
   \printcontents{}{1}{\setcounter{tocdepth}{2}}
\end{multicols}

This is dangerously familiar ground for physicists. A dynamical system is some system:

\begin{equation}
\partial_t\mathbf{x} = \mathbf{f}(\mathbf{x},t,\mathbf{u};\mathbf{\beta})
\end{equation}

Where $\mathbf{x}$ are the state space coordinates of the system at some time $t$, and $\mathbf{f}$ is the \textit{dynamics} of the system. $\mathbf{u}$ is some control input and $\mathbf{\beta}$ are parameters. A system where $\mathbf{f}$ depends on time is called \textit{non-autonomous} and a system that has an $\mathbf{f}$ that does not depend on time is called \textit{autonomous}.  

Conventionally, the dynamics $\mathbf{f}$ are derived from first principles. Increasingly, it is possible to infer them from data. Challenges arise from:

\begin{itemize}
\item Nonlinear $\mathbf{f}$, that is, the system cannot be described in the form $\partial_t\mathbf{x}=\mathbf{Ax}$
\item Unknown $\mathbf{f}$
\item High dimensional state vector $\mathbf{x}$
\item Chaos, Transients
\item Noise, Stochastic forcing functions
\item Multiscale dynamics
\item Uncertainty (in parameters etc.)
\end{itemize}

\section{Mode Decompositions}
Modal decompositions express the evolving state of a system in terms of a superposition of basis vectors that are the eigenfunctions of a time-translation operator. In general, the rank of the basis space is the same as the state space dimension, but in practice only a few of those modes have "energy". That means that the description of a system in terms of a modal decomposition often requires orders of magnitude fewer parameters than the description of the state space. Extracting the dominant modes also amounts to extracting the dominant dynamics of the system, with the assumption possibly being that the discarded low-energy modes are noise. In so far, modal decomposition runs closely parallel to decomposition-based dimensionality reduction techniques such as SVD, PCA and ICA, though explicitly introducing the notion of a trajectory through time.

The modes are properties of the time translation operator and the state space. Therefore, the description in terms of modes is compatible with an arbitrary, unpredictable forcing function acting on the system, so long as it does not affect the time translation operator or the state space. For example, the description of the oscillation of a guitar string in terms of modes remains valid regardless of the song that is being played. However, changing the length of the string increases the size of the state space, and increasing the tension in the string accelerates its reversion from being struck away from its equilibrium, and therefore it's time translation operator. In those cases the modes of the system are changed.   


\section{Koopman and Frobenius-Perron Operators}
The Koopman Operator, or composition operator, is the classical analogue of the time translation operator $e^{-i\mathbf{\hat{H}}t/\hbar}$ in quantum mechanics. It describes dynamics in terms of the \textit{Heisenberg Picture}, in which operators, rather than states, have time-dependence. It describes autonomous systems (in quantum mechanics, that would be a system where the Hamiltonian $\mathbf{\hat{H}}$ nor the size of the state space depend on time).

Consider a dynamical system evolving on a manifold $\mathscr{M}$, so that, in discrete time:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_{k})
\end{equation}

Where $\mathbf{x}\in\mathscr{M}$ are the state space coordinates of the system at a given time. Given a scalar valued function on the state space $g:\mathscr{M} \rightarrow \mathbb{R}$, the Koopman Operator $\mathbf{\hat{K}}$ acts as:

\begin{equation}
\mathbf{\hat{K}}g(\mathbf{x}) = g(\mathbf{f}(\mathbf{x}))
\end{equation} 

Which acts as time translation by one step.

The Koopman Operator is infinite dimensional and linear, even though the dynamical system might be nonlinear with a finite state space. The Frobenius-Perron Operator (sometimes also Ruelle Operator, Ruelle-Frobenius-Perron Operator, or Transfer Operator) is the right adjoint of the Koopman operator, so that an approximation of one provides an approximation of the other. Rather than a function in state space, it translates the state space density in time. Let $\mathbf{\hat{P}}$ be the Frobenius-Perron Operator. Let the state space density be $\rho(\mathbf{x}$, and define some operator $\mathbf{\hat{A}}$, so that $\mathbf{\hat{A}}\rho(\mathbf{x}) = g(\mathbf{x})$. Then, in terms of either the Koopman or the Frobenius-Perron approach, the average value of the quantity $\left< g(\mathbf{x}(t))\right>$ at time $t$ in the Koopman picture or in the Frobenius-Perron picture is \cite{cvitanovic2016chaos,salova2019koopman}:

\begin{equation}
\begin{array}{rl}
\left< g(\mathbf{x}(t))\right> =& \int_\mathscr{M} \mathrm{d}x \mathbf{\hat{K}}(t)\mathbf{\hat{A}}\rho(\mathbf{x}) \\
& \int_\mathscr{M} \mathrm{d}x \mathbf{\hat{A}}\mathbf{\hat{P}}(t)\rho(\mathbf{x})
\end{array}
\end{equation}

Which implies that $\mathbf{\hat{K}}(t)\mathbf{\hat{A}} \left[\mathbf{\hat{P}}(t)\right]^{-1} = \left[\mathbf{\hat{K}}(t) \right]^{-1} \mathbf{\hat{A}} \mathbf{\hat{P}}(t) = \mathbf{\hat{A}}$.


The function $g(\mathbf{x}(t))$ is an \textit{observable} of the system, which might be a pixel value, the value of a stock portfolio, the energy of a particle, or something like that. The state space coordinates $\mathbf{x}(t)$ may or may not correspond to quantities that are actually observable. Their role is to uniquely parametrize the possible states of the system. The Koopman Operator acts on observables, which is probably why it seems to come up more often in data-driven work. 


% POD 

\section{Proper Orthogonal Decomposition (POD)}
Proper Orthogonal Decomposition (POD) seeks a lower-rank representation of a dynamical system that, as far as I can see, is entirely analogous to SVD or PCA \cite{megretski2004pod}. Given a system $\mathbf{x}(t) \in \mathbb{R}^n$, one looks for a low-rank projection $\Pi_r$ that minimizes the expected value of the error:

\begin{equation}
\argmin \mathbb{E}_t||\mathbf{x}(t) - \mathbf{\Pi}\mathbf{x}(t)||_{2}
\end{equation} 

This is the same loss function as for PCA, and so the projection matrix $\mathbf{\Pi}_r = \sum_{i=1}^{r} \mathbf{v}_i\mathbf{v}^T_i$ where $\mathbf{v}_i$ is the $i$th principal component vector. 

The POD does not have anything to say about the dynamics of a system, indeed, the time-ordering of the snapshots has no effect on the result. As such, performing POD on a system across different time windows should yield different decompositions unless the system is completely at rest, even when the dynamics of the system are steady. The first orthogonal component is simply the point in state space that the system is most correlated with across the observed time-frame.  

% DMD
\section{Dynamic Mode Decomposition (DMD)}
\label{sec:dmd}
Dynamic Mode Decomposition (DMD) fits a reduced-rank, linear dynamical system to multi-dimensional time-series data. The rank-reduction is useful when the state space is very high-dimensional, for example when each time step is an image with many pixels. Canonically, fitting and rank reduction relies on SVD, so that the method is based on $L^2$ loss. It originated in the fluid dynamics community, where very high-dimensional state spaces are common because dynamics must be resolved across many length scales.

In case of plain-vanilla DMD, the dataset consists of pairs of snapshots of the system, which show the system at two subsequent time steps, i.e. $\{(\mathbf{x}_t,\mathbf{y}_t): \mathbf{y}_t = \mathbf{x}_{t+\delta t} \}$. Often, the different pairs show the system on different state space trajectories. That is, they may originate with many different "incarnations" of the system. To me it looks basically like an VAR(1) model, except that it can handle a very high dimensional state vector by extracting the leading eigendecomposition of the coefficient matrix without having to calculate it. It also looks a lot like fitting the transition matrix of a Markov Chain.

Given a set of $m$ snapshots of the system $\{\mathbf{x}_t:\ t\in[1,m], \mathbf{x}_t \in \mathbb{R}^{n}\}$, let $\mathbf{X}_{1,m-1} = \left[\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_{t-1},\mathbf{x}_{m-1}\right]$ be the matrix of state vectors from $t\in[1,m-1]$ and $\mathbf{X}_{2,m}$ be the matrix of state vectors advanced by one time step from $t\in[2,m]$. Then dynamic mode decomposition essentially looks for linear dynamics using linear regression:

\begin{equation}
\mathbf{X}_{2,m} = \mathbf{A}\mathbf{X}_{1,m-1}
\end{equation}

Where $\mathbf{A} \in \mathbb{R}^{n\times n}$ is square and therefore diagonalizable, but may be very large.

Let $a_{i,j}$ be the entry in the $i$th row and $j$th column of $\mathbf{A}$. Elementwise, the equation for the $i$th state variable at time $t$, $x_{i,t} = [\mathbf{x}_t]_i$ is written:

\begin{equation}
x_{i,t} = \sum_j a_{i,j} x_{i,t-1}
\end{equation}

The eigenvectors of $\mathbf{A}$  correspond to normal modes of the system. The corresponding eigenvectors, which may be real or complex, predict the evolution of the mode over time.

The size of $\mathbf{A}\in\mathbb{R}^{n\times n}$ may be so large that extracting its eigenvectors and eigenvalues may be computationally intractable. DMD instead derives a smaller matrix $\mathbf{A'}$ that has the same eigenvalues as $\mathbf{A}$ and uses those to also finds the eigenvectors.

\begin{equation}
\begin{array}{ll}
1. & \mathbf{X}_{1,m-1} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T,\ \ \ \ \mathbf{X}_{2,m} = \mathbf{AU\Sigma V}^T\\
2. & \mathbf{U}^T\mathbf{X}_{2,m}\mathbf{V\Sigma}^{-1} = \mathbf{U}^{T}\mathbf{AU} = \mathbf{A'}\\
3. & \mathbf{A'}\mathbf{W} = \mathbf{W}\mathbf{\Lambda} \\
4. & \mathbf{\Phi} = \mathbf{X}^T\mathbf{V\Sigma}^{-1}\mathbf{W}
\end{array}
\end{equation}
 
If the system has low dimensional structure, the matrix $\mathbf{A'}$ can be further reduced by only keeping the first $r$ vectors in $\mathbf{U}$. The final transformation$\mathbf{\Phi}$ gives high-dimensional eigenvectors of the full matrix $\mathbf{A}$. 

\citeasnoun{stevebrunton} suggests that DMD is, in spirit, a combination of principal component analysis and the Fourier transform in time. The fact that DMD exists in a regression framework there are a huge number of extensions that can be leveraged. In particular, \citeasnoun{stevebrunton} delves into combining DMD with compressed sensing. What is surprising is that the dynamics that are modeled by DMD are not necessarily linear. The matrix $\mathbf{A}$ is large enough to approximate nonlinear dynamics.


\section{Extended Dynamical Mode Decomposition (EDMD)}
\label{sec:edmd}


\section{Sparse Identification of Nonlinear Dynamics (SINDy)}
\label{sec:sindy}

\section{DMD with Irregularly Sampled Timesteps}
