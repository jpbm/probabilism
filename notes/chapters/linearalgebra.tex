% \chapauthor{J. P. Balthasar Mueller}
\chapter{Linear Algebra}

Resolution for the new year: every time you have to look something up, try to make a note of it.

\begin{multicols}{2}[\subsubsection*{Contents of this chapter}]
   \printcontents{}{1}{\setcounter{tocdepth}{2}}
\end{multicols}

\section{Spectral Theorem}

\section{Singular Value Decomposition}





\section{Cholesky Decomposition}
The Cholesky Decomposition exists when a matrix is hermitian and positive-definite. It expresses the matrix $\mathbf{A}$ as:

\begin{equation}
\mathbf{A} = \mathbf{L}\mathbf{L^\dagger}
\end{equation}

Where $\mathbf{L}$ is a lower-triangular matrix with positive, real diagonal entries. The Cholesky decompositions 

\section{Generalized Eigenvectors}

\section{Affine Transformations}

Affine transformations are the combination of a linear map and a translation, which has the form $f(\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{b}$. 

\begin{equation}
f: V \rightarrow W
\end{equation}

Where $V$ and $W$ are vector spaces. Affine transformations can be expresses as matrices by adding an entry with a constant to the vectors that describe a point in space. For example, for $\mathbf{x} \in \mathbb{R}^n$,  the affine transform $f(\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{b}$ with $A\in\mathbb{R}^{n,n}$ and $x,b \in \mathbb{R}^{n}$ can be expressed as the product of a rectangular matrix $\mathbf{M}$ and a vector $\mathbf{c}$ as:

\begin{equation}
\mathbf{A}\mathbf{x} + \mathbf{b} = \underbrace{\left[\begin{array}{c|c} \mathbf{A} & \mathbf{b} \end{array}\right]}_{\mathbf{M}} \underbrace{\left[\begin{array}{c} \mathbf{x} \\ 1\end{array} \right]}_{\mathbf{c}}
\end{equation}

Where $\mathbf{c}^T = \left[x_1,x_2,x_3,...,x_n,1\right]$ and $\mathbf{M} \in \mathbb{R}^{n,n+1}$.

\section{Multilinear Maps}

A multilinear map acts on several vectors in a way that is linear in each of its arguments. A $k$-linear map acts on $k$ vectors, where $k=2$ are bilinear maps and $k=1$ are linear maps.

\begin{equation}	
f: V_1 \times V_2 \times ... \times V_n \rightarrow W
\end{equation}

Where $V_1, V_2, ... , V_n$ and $W$ are vector spaces. An example would be the addition or subtraction of two or more vectors.

\section{Multilinear Forms}
Multilinear forms are multilinear maps that have a scalar output. An example is the dot product between two vectors, or summing over the elements of one or more vectors.

\begin{equation}
f: V_1 \times V_2 \times ... \times V_n \rightarrow K
\end{equation}

Where $V_1, V_2, ... , V_n$ and $K$ is a scalar field.


\section{Types of Matrices}

\subsection{Hermitian}
Hermitian matrices are matrices that are equal to their complex transpose. That is:

\begin{equation}
{\mathbf{A}^{*}}^T = \mathbf{A}^\dagger = \mathbf{A} 
\end{equation}

Hermitian matrices are like a generalization of symmetric matrices to include complex numbers. 

\subsubsection{Properties}
There are many properties. 
\begin{itemize}
\item By definition: $\mathbf{A} = \mathbf{A}^\dagger$
\item Diagonal Entries are all real, since $a_{i,i} = a_{i,i}^*$
\item Inverse is also hermitian:  $\mathbf{A}^{-1} ={ \mathbf{A}^{-1}}^\dagger$
\item Diagonalizable with real eigenvalues and orthogonal eigenvectors $\in \mathbb{C}^n$.
\end{itemize}


\subsection{Triangular}
A lower triangular matrix is a matrix that has all-zero entries above the diagonal.

\begin{equation}
\mathbf{L} = \left[\begin{array}{cccccc} l_{1,1}&&&&&0\\l_{2,1}&l_{2,2}&&&&\\l_{3,1}&l_{3,2}&\ddots&&&\\  \vdots&\vdots&\ddots&\ddots&&\\ \vdots&\vdots&&\ddots&\ddots&\\  l_{n,1}&l_{n,2}&\hdots&\hdots&l_{n,n-1}&l_{n,n}\end{array}\right]
\end{equation}

Upper triangular matrices are matrices that have all-zero entries below the diagonal.

\section{Taking Derivatives}

\begin{equation}
\begin{array}{l}
\frac{d}{d\mathbf{x}} \left(\mathbf{u}^T\mathbf{x}\right) = \left[\frac{d}{dx_1}\left(\sum_i u_i x_i\right),...,\frac{d}{dx_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{x}^T\mathbf{u}\right) = \left[\frac{d}{dx_1}\left(\sum_i u_i x_i\right),...,\frac{d}{dx_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{x}^T\mathbf{x}\right) = \left[\frac{d}{dx_1}\left(\sum_i x_i^2\right),...,\frac{d}{dx_n}\left(\sum_i x_i^2\right)\right] = 2\mathbf{x}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{A}x\right) = \left[
\begin{array}{ccc} 
\underbrace{\frac{d}{dx_1}\left(\sum_i A_{1i} x_i\right)}_{A_{11}} &...& \underbrace{\frac{d}{dx_n}\left(\sum_i A_{1i} x_i\right)}_{A_1n}\\
\vdots&\vdots&\vdots\\
\underbrace{\frac{d}{dx_1}\left(\sum_i A_{ni} x_i\right)}_{A_{n1}} &...& \underbrace{\frac{d}{dx_n}\left(\sum_i A_{ni} x_i\right)}_{A_{nn}}\\
\end{array}\right] = \mathbf{A}\\
\end{array}
\end{equation}


\chapauthor{}
