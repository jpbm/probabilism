% \chapauthor{J. P. Balthasar Mueller}
\chapter{Linear Algebra}

\begin{multicols}{2}[\subsubsection*{Contents of this chapter}]
   \printcontents{}{1}{\setcounter{tocdepth}{2}}
\end{multicols}

\section{Spectral Theorem}

\section{Singular Value Decomposition}




% Cholesky Decomposition
\section{Cholesky Decomposition}
\label{sec:cholesky}

The Cholesky Decomposition exists when a matrix is hermitian and positive-definite. It expresses the matrix $\mathbf{A}$ as:

\begin{equation}
\mathbf{A} = \mathbf{L}\mathbf{L^\dagger}
\end{equation}

Where $\mathbf{L}$ is a lower-triangular matrix with positive, real diagonal entries. When $\mathbf{A}$ is real, then so is $\mathbf{L}$. The Cholesky decomposition enables fast solution of a linear system, but it can also be used to create correlated random variables in Monte Carlo simulations. 

\subsubsection{Creating Correlated Random Variables}
Let $\mathbf{u}_t$ be a vector of uncorrelated samples with sandard deviation 1. If the covariance matrix of the system to be simulated is  $\mathbf{\Sigma}$ with Cholesky decomposition $\mathbf{\Sigma} = \mathbf{LL}^\dagger$, then the vector $\mathbf{v}_t = \mathbf{Lu}_t$ has the desired covariance.

\begin{figure}
\centering
\includegraphics[scale=0.5]{cholesky1.png}
\includegraphics[scale=0.5]{cholesky2.png}
\caption{Creating correlated random variables from uncorrelated random variables using the Cholesky decomposition of the covariance matrix. The 5 uncorrelated random variables are sampled from a standard normal distribution. It is difficult to see a difference between the correlated and uncorrelated random walks.}
\end{figure}

\section{Non-Negative Matrix Factorization}
Non-Negative Matrix Factorization works for positive matrices and is interesting as a method for dimensionality reduction. It works by expressing a matrix $\mathbf{X}\in\mathbb{R}^{p\times n}_+$ in terms of two smaller positive matrices $\mathbf{W}\in\mathbb{R}_+^{p\times r}$ and $\mathbf{H}\in \mathbb{R}^{r\times n}_+$. An excellent introduction is in \possessivecite{morningpaper2019nnmf} blog post and in \citeasnoun{gillis2014and}. 


\section{Generalized Eigenvectors}


\section{Affine Transformations}

Affine transformations are the combination of a linear map and a translation, which has the form $f(\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{b}$. 

\begin{equation}
f: V \rightarrow W
\end{equation}

Where $V$ and $W$ are vector spaces. Affine transformations can be expresses as matrices by adding an entry with a constant to the vectors that describe a point in space. For example, for $\mathbf{x} \in \mathbb{R}^n$,  the affine transform $f(\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{b}$ with $A\in\mathbb{R}^{n,n}$ and $x,b \in \mathbb{R}^{n}$ can be expressed as the product of a rectangular matrix $\mathbf{M}$ and a vector $\mathbf{c}$ as:

\begin{equation}
\mathbf{A}\mathbf{x} + \mathbf{b} = \underbrace{\left[\begin{array}{c|c} \mathbf{A} & \mathbf{b} \end{array}\right]}_{\mathbf{M}} \underbrace{\left[\begin{array}{c} \mathbf{x} \\ 1\end{array} \right]}_{\mathbf{c}}
\end{equation}

Where $\mathbf{c}^T = \left[x_1,x_2,x_3,...,x_n,1\right]$ and $\mathbf{M} \in \mathbb{R}^{n,n+1}$.

\section{Multilinear Maps}

A multilinear map acts on several vectors in a way that is linear in each of its arguments. A $k$-linear map acts on $k$ vectors, where $k=2$ are bilinear maps and $k=1$ are linear maps.

\begin{equation}	
f: V_1 \times V_2 \times ... \times V_n \rightarrow W
\end{equation}

Where $V_1, V_2, ... , V_n$ and $W$ are vector spaces. An example would be the addition or subtraction of two or more vectors.

\section{Multilinear Forms}
Multilinear forms are multilinear maps that have a scalar output. An example is the dot product between two vectors, or summing over the elements of one or more vectors.

\begin{equation}
f: V_1 \times V_2 \times ... \times V_n \rightarrow K
\end{equation}

Where $V_1, V_2, ... , V_n$ and $K$ is a scalar field.


\input{./chapters/sections/linalg_matrixtypes.tex}
\input{./chapters/sections/linalg_norms.tex}

\section{Taking Derivatives}

\begin{equation}
\begin{array}{l}
\frac{d}{d\mathbf{x}} \left(\mathbf{u}^T\mathbf{x}\right) = \left[\frac{d}{dx_1}\left(\sum_i u_i x_i\right),...,\frac{d}{dx_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{x}^T\mathbf{u}\right) = \left[\frac{d}{dx_1}\left(\sum_i u_i x_i\right),...,\frac{d}{dx_n}\left(\sum_i u_i x_i\right)\right] = \mathbf{u}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{x}^T\mathbf{x}\right) = \left[\frac{d}{dx_1}\left(\sum_i x_i^2\right),...,\frac{d}{dx_n}\left(\sum_i x_i^2\right)\right] = 2\mathbf{x}^T\\
\\
\frac{d}{d\mathbf{x}} \left(\mathbf{A}x\right) = \left[
\begin{array}{ccc} 
\underbrace{\frac{d}{dx_1}\left(\sum_i A_{1i} x_i\right)}_{A_{11}} &...& \underbrace{\frac{d}{dx_n}\left(\sum_i A_{1i} x_i\right)}_{A_1n}\\
\vdots&\vdots&\vdots\\
\underbrace{\frac{d}{dx_1}\left(\sum_i A_{ni} x_i\right)}_{A_{n1}} &...& \underbrace{\frac{d}{dx_n}\left(\sum_i A_{ni} x_i\right)}_{A_{nn}}\\
\end{array}\right] = \mathbf{A}\\
\end{array}
\end{equation}


\chapauthor{}
